#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun  5 15:31:49 2025
@author: zsolt
"""

import mwclient
import os
import json
from pathlib import Path
import configparser

CONFIG_PATH = 'wiki_rag.conf'
DEFAULT_OUTPUT = 'data/wiki_pages.json'

def load_config(path=CONFIG_PATH):
    config = configparser.ConfigParser()
    config.read(path)
    return config

def save_pages(pages, output_path):
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as file:
        json.dump(pages, file, ensure_ascii=False, indent=2)
    print(f"‚úÖ Let√∂ltve: {len(pages)} oldal --> {output_path}")

def connect(site_url, path, username=None, password=None):
    print(f"üîó Csatlakoz√°s: https://{site_url}{path}")
    site = mwclient.Site(site_url, path=path)
    if username and password:
        site.login(username, password)
        print("üîê Bejelentkez√©s sikeres")
    return site

def fetch_wiki_pages(site_url, path='/wiki/', username=None, password=None, limit=50, output_path=DEFAULT_OUTPUT):
    site = connect(site_url, path, username, password)
    pages = []
    for i, page in enumerate(site.allpages()):
        if i >= limit:
            break
        try:
            text = page.text()
            pages.append({'title': page.name, 'text': text})
        except Exception as error:
            print(f"‚ùå Skipping {page.name}: {error}")
    save_pages(pages, output_path)

def fetch_selected_pages(site_url, titles, path='/w/', username=None, password=None):
    site = mwclient.Site(site_url, path=path)
    if username and password:
        site.login(username, password)
        print("üîê Bejelentkez√©s sikeres")

    pages = []
    print(f"üîó Csatlakoz√°s: https://{site_url}{path}")
    print(f"üìÑ Let√∂ltend≈ë oldalak: {titles}")

    for title in titles:
        try:
            print(f"üîÑ Let√∂lt√©s: {title}")
            page = site.pages[title]

            # Ellen≈ërizz√ºk, hogy l√©tezik-e az oldal
            if not page.exists:
                print(f"‚ö†Ô∏è Az oldal nem l√©tezik: {title}")
                continue

            text = page.text()
            if text.strip():  # Ellen≈ërizz√ºk, hogy van-e tartalom
                pages.append({
                    'title': title,
                    'text': text
                })
                print(f"‚úÖ Sikeresen let√∂ltve: {title} ({len(text)} karakter)")
            else:
                print(f"‚ö†Ô∏è √úres oldal: {title}")
        except Exception as error:
            print(f"‚ùå Hiba '{title}' let√∂lt√©se k√∂zben: {error}")

    # Ment√©s √©s eredm√©ny ki√≠r√°sa
    if pages:
        os.makedirs(os.path.dirname(DEFAULT_OUTPUT), exist_ok=True)
        with open(DEFAULT_OUTPUT, 'w', encoding='utf-8') as file:
            json.dump(pages, file, ensure_ascii=False, indent=2)
        print(f"‚úÖ √ñsszesen let√∂ltve: {len(pages)} oldal --> {DEFAULT_OUTPUT}")
    else:
        print("‚ùå Nem siker√ºlt egyetlen oldalt sem let√∂lteni.")

def fetch_related_pages(site_url, root_title, limit=50, path='/w/', username=None, password=None):
    site = mwclient.Site(site_url, path=path)
    if username and password:
        site.login(username, password)
        print("üîê Bejelentkez√©s sikeres")

    print(f"üîó Csatlakoz√°s: https://{site_url}{path}")
    print(f"üîç Keres√©s: '{root_title}' kezdet≈± oldalak")

    try:
        results = site.api('query', list='prefixsearch', pssearch=root_title, pslimit=limit)
        titles = [res['title'] for res in results.get('query', {}).get('prefixsearch', [])]

        if not titles:
            print(f"‚ö†Ô∏è Nincs tal√°lat: '{root_title}' kezdet≈± oldalakra.")
            return

        print(f"üîπ Tal√°lt oldalak ({len(titles)}): {titles}")
        fetch_selected_pages(site_url, titles, path=path, username=username, password=password)

    except Exception as error:
        print(f"‚ùå Hiba prefixsearch k√∂zben: {error}")

def fetch_selected_pages_return(site_url, titles, path='/w/', username=None, password=None):
    """
    Kiv√°lasztott oldalak let√∂lt√©se √©s visszaad√°sa (ment√©s n√©lk√ºl)
    """
    site = mwclient.Site(site_url, path=path)
    if username and password:
        site.login(username, password)
        print("üîê Bejelentkez√©s sikeres")

    pages = []
    print(f"üîó Csatlakoz√°s: https://{site_url}{path}")
    print(f"üìÑ Let√∂ltend≈ë oldalak: {titles}")

    for title in titles:
        try:
            print(f"üîÑ Let√∂lt√©s: {title}")
            page = site.pages[title]

            if not page.exists:
                print(f"‚ö†Ô∏è Az oldal nem l√©tezik: {title}")
                continue

            text = page.text()
            if text.strip():
                pages.append({
                    'title': title,
                    'text': text
                })
                print(f"‚úÖ Sikeresen let√∂ltve: {title} ({len(text)} karakter)")
            else:
                print(f"‚ö†Ô∏è √úres oldal: {title}")
        except Exception as error:
            print(f"‚ùå Hiba '{title}' let√∂lt√©se k√∂zben: {error}")

    return pages

def fetch_related_pages_return(site_url, root_title, limit=50, path='/w/', username=None, password=None):
    """
    Kapcsol√≥d√≥ oldalak let√∂lt√©se √©s visszaad√°sa (ment√©s n√©lk√ºl)
    """
    site = mwclient.Site(site_url, path=path)
    if username and password:
        site.login(username, password)
        print("üîê Bejelentkez√©s sikeres")

    print(f"üîó Csatlakoz√°s: https://{site_url}{path}")
    print(f"üîç Keres√©s: '{root_title}' kezdet≈± oldalak")

    try:
        results = site.api('query', list='prefixsearch', pssearch=root_title, pslimit=limit)
        titles = [res['title'] for res in results.get('query', {}).get('prefixsearch', [])]

        if not titles:
            print(f"‚ö†Ô∏è Nincs tal√°lat: '{root_title}' kezdet≈± oldalakra.")
            return []

        print(f"üîπ Tal√°lt oldalak ({len(titles)}): {titles}")
        return fetch_selected_pages_return(site_url, titles, path=path, username=username, password=password)

    except Exception as error:
        print(f"‚ùå Hiba prefixsearch k√∂zben: {error}")
        return []

def auto_fetch_from_config(conf_file='wiki_rag.conf'):
    config = configparser.ConfigParser()

    if not os.path.exists(conf_file):
        print(f"‚ùå Konfigur√°ci√≥s f√°jl nem tal√°lhat√≥: {conf_file}")
        return

    config.read(conf_file)

    if not config.has_section('wiki') or not config.get('wiki', 'url', fallback='').strip():
        print("‚ùå A 'wiki' szekci√≥ vagy az 'url' hi√°nyzik a konfigur√°ci√≥b√≥l.")
        return

    site_url = config.get('wiki', 'url').strip()
    path = config.get('wiki', 'path', fallback='/w/').strip()
    username = config.get('wiki', 'username', fallback=None)
    password = config.get('wiki', 'password', fallback=None)

    print(f"üìã Konfig bet√∂ltve: {site_url}")

    # selected pages
    selected = config.get('selected', 'pages', fallback='').strip() if config.has_section('selected') else ''
    selected_pages = [p.strip() for p in selected.split(',') if p.strip()]

    # related pages
    related_root = config.get('related', 'root', fallback='').strip() if config.has_section('related') else ''
    related_limit_str = config.get('related', 'limit', fallback='50').strip() if config.has_section('related') else '50'
    related_limit = int(related_limit_str) if related_limit_str.isdigit() else 50

    all_pages = []  # K√∂z√∂s lista az √∂sszes oldal sz√°m√°ra

    if selected_pages:
        print(f"üìù Kiv√°lasztott oldalak let√∂lt√©se: {selected_pages}")
        selected_data = fetch_selected_pages_return(site_url, selected_pages, path=path, username=username, password=password)
        all_pages.extend(selected_data)

    if related_root:  # V√°ltoz√°s: elif helyett if
        print(f"üîó Kapcsol√≥d√≥ oldalak let√∂lt√©se: '{related_root}' gy√∂k alapj√°n")
        related_data = fetch_related_pages_return(site_url, related_root, limit=related_limit, path=path, username=username, password=password)
        all_pages.extend(related_data)

    if not selected_pages and not related_root:
        print("‚ö†Ô∏è Nincs megadva let√∂ltend≈ë oldal a [selected] vagy [related] szekci√≥ban.")
        return

    # V√©gs≈ë ment√©s
    if all_pages:
        os.makedirs(os.path.dirname(DEFAULT_OUTPUT), exist_ok=True)
        with open(DEFAULT_OUTPUT, 'w', encoding='utf-8') as file:
            json.dump(all_pages, file, ensure_ascii=False, indent=2)
        print(f"‚úÖ √ñsszesen let√∂ltve: {len(all_pages)} oldal --> {DEFAULT_OUTPUT}")
    else:
        print("‚ùå Nem siker√ºlt egyetlen oldalt sem let√∂lteni.")

# P√©lda haszn√°lat
if __name__ == "__main__":
    auto_fetch_from_config()